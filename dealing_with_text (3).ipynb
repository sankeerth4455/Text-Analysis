{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fd7fd9e"
      },
      "source": [
        "# Dealing with text\n",
        "\n",
        "In general, deep learning models need numeric input. The bulk of the digital information in the world, however, is in text format. So, we need a way to convert from text to numbers.\n",
        "\n",
        "The usual sequence for this is as follows:\n",
        " - standardization\n",
        " - tokenization\n",
        " - indexing\n",
        " - encoding/embedding\n",
        "\n",
        "<img src=\"text_process.png\" width=600 align=\"center\">\n",
        "\n",
        "(image source: *Deep Learning with Python, 2nd edition, F. Chollet*)"
      ],
      "id": "7fd7fd9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bd27a69"
      },
      "source": [
        "## Standardization"
      ],
      "id": "3bd27a69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4f0311"
      },
      "source": [
        "### Punctuation"
      ],
      "id": "3e4f0311"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2b8c7529",
        "outputId": "44689c8c-1a56-47e5-afc8-30d933c5fe1e"
      },
      "source": [
        "import string\n",
        "\n",
        "string.punctuation"
      ],
      "id": "2b8c7529",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c52ab1f7",
        "outputId": "551b3e60-db25-4e4f-d32d-d12daa142223"
      },
      "source": [
        "string_with_punc = \"This string, and by no means is it the only one, has some punctuation.\"\n",
        "\n",
        "print(f\"Remove puncuation:\\n{str([c for c in string_with_punc if c not in string.punctuation])}\\n\")\n",
        "\n",
        "print(f\"Reform sentence by joining:\\n{''.join(c for c in string_with_punc if c not in string.punctuation)}\")"
      ],
      "id": "c52ab1f7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remove puncuation:\n",
            "['T', 'h', 'i', 's', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 'b', 'y', ' ', 'n', 'o', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 's', ' ', 'i', 't', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 'o', 'n', 'e', ' ', 'h', 'a', 's', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'u', 'n', 'c', 't', 'u', 'a', 't', 'i', 'o', 'n']\n",
            "\n",
            "Reform sentence by joining:\n",
            "This string and by no means is it the only one has some punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb83a098"
      },
      "source": [
        "### Lower case"
      ],
      "id": "cb83a098"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "553fc06c",
        "outputId": "9fa4e209-f47c-4e31-9897-53e3da608a12"
      },
      "source": [
        "string_with_upper = \"THIs iS A meSSeD uP SeNTenCE\"\n",
        "\n",
        "string_lower = string_with_upper.lower()\n",
        "string_lower"
      ],
      "id": "553fc06c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is a messed up sentence'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61cde069"
      },
      "source": [
        "### Special characters"
      ],
      "id": "61cde069"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9133be65",
        "outputId": "0a2f7edc-1944-470b-cfc7-8d6f562ca972"
      },
      "source": [
        "!pip install nltk"
      ],
      "id": "9133be65",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e463d4c2",
        "outputId": "3283e02e-d502-4b56-92ad-ed2cf4427bbe"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "id": "e463d4c2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "028e06a7",
        "outputId": "93e47d5d-97ec-4af1-b11d-23d16d80650e"
      },
      "source": [
        "import unidecode\n",
        "\n",
        "accent = u'México'\n",
        "\n",
        "no_accent = unidecode.unidecode(accent)\n",
        "no_accent"
      ],
      "id": "028e06a7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f90f9dde3eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'México'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mno_accent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37668b12"
      },
      "source": [
        "### Stemming"
      ],
      "id": "37668b12"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "935e8fc7"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"study\", \"studies\", \"studying\", \"studied\"]\n",
        "\n",
        "for w in words:\n",
        "    print(f\"Original word:  {w}, after stemming:  {stemmer.stem(w)}\")"
      ],
      "id": "935e8fc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc246b1a"
      },
      "source": [
        "### Lemmatization"
      ],
      "id": "cc246b1a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94fc8f28"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for w in words:\n",
        "    print(f\"Original word: {w}, after lemmatization: {lemmatizer.lemmatize(w, pos='v')}\")"
      ],
      "id": "94fc8f28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ea8bdd"
      },
      "source": [
        "## Tokenization"
      ],
      "id": "d7ea8bdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b50b2a5"
      },
      "source": [
        "### Character-level"
      ],
      "id": "5b50b2a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6277abc2"
      },
      "source": [
        "str(list(string_lower)) # str() is only used here to get the list to print nicely across the screen"
      ],
      "id": "6277abc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "becb033a"
      },
      "source": [
        "### Word-level"
      ],
      "id": "becb033a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79e3f036"
      },
      "source": [
        "string_lower.split()"
      ],
      "id": "79e3f036",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c806ef3a"
      },
      "source": [
        "### N-grams"
      ],
      "id": "c806ef3a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "137c438e"
      },
      "source": [
        "# !pip install nltk  # uncomment and install nltk if needed"
      ],
      "id": "137c438e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fab17a8"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "for bigram in ngrams(string_lower.split(), 2):\n",
        "    print(bigram)"
      ],
      "id": "2fab17a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2b8c36b"
      },
      "source": [
        "## Vocabulary indexing"
      ],
      "id": "a2b8c36b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3416fc1"
      },
      "source": [
        "vocabulary = {}\n",
        "\n",
        "text = string_lower.split()\n",
        "\n",
        "for token in text:\n",
        "    if token not in vocabulary:\n",
        "        vocabulary[token] = len(vocabulary)"
      ],
      "id": "d3416fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5876e41",
        "outputId": "d3e9958d-5b07-48f5-bffb-a5f95afdd675"
      },
      "source": [
        "vocabulary"
      ],
      "id": "d5876e41",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 2, 'is': 1, 'messed': 3, 'sentence': 5, 'this': 0, 'up': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10209037"
      },
      "source": [
        "The vocabulary is created using the training data, so one thing that can occur is that there will be words in your test data that are not in your vocabulary. We will assign a vocabulary entry of {\"[UNK]\": 0} to these \"unknown\" words.\n",
        "Thus, we can adjust our code above to the following:"
      ],
      "id": "10209037"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f986fac4",
        "outputId": "7e038fa0-9ba6-4732-a1f3-d7bbcd534959"
      },
      "source": [
        "vocabulary = {\"[UNK]\": 0}\n",
        "\n",
        "text = string_lower.split()\n",
        "\n",
        "for token in text:\n",
        "    if token not in vocabulary:\n",
        "        vocabulary[token] = len(vocabulary)\n",
        "\n",
        "vocabulary"
      ],
      "id": "f986fac4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[UNK]': 0, 'a': 3, 'is': 2, 'messed': 4, 'sentence': 6, 'this': 1, 'up': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6736fa81"
      },
      "source": [
        "You can now encode sentences using this vocabulary."
      ],
      "id": "6736fa81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dde54b",
        "outputId": "b11edb45-e43e-44c7-9180-3758bb2e84cf"
      },
      "source": [
        "test_sentence = \"This sentence is not messed up.\"\n",
        "test_sentence = test_sentence.lower()\n",
        "test_sentence = \"\".join(char for char in test_sentence if char not in string.punctuation)\n",
        "test_sentence = test_sentence.split()\n",
        "test_sentence"
      ],
      "id": "b1dde54b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'sentence', 'is', 'not', 'messed', 'up']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a4f6bd",
        "outputId": "ef7ef1e4-c18f-405c-c5da-c2f2a9ae757d"
      },
      "source": [
        "encoded = [vocabulary.get(token, 0) for token in test_sentence]\n",
        "encoded"
      ],
      "id": "c9a4f6bd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 6, 2, 0, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "022ca8ed"
      },
      "source": [
        "We will also want the ability to decode text that has been encoded. To do this, we create an inverse vocabulary:"
      ],
      "id": "022ca8ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d08ff461",
        "outputId": "76d9a48b-c5be-4e78-a596-54a8af639634"
      },
      "source": [
        "inverse_vocabulary = {}\n",
        "\n",
        "for k, v in vocabulary.items():\n",
        "    inverse_vocabulary[v] = k\n",
        "\n",
        "inverse_vocabulary"
      ],
      "id": "d08ff461",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '[UNK]', 1: 'this', 2: 'is', 3: 'a', 4: 'messed', 5: 'up', 6: 'sentence'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ca48061"
      },
      "source": [
        "Now if we have encoded text, we can get the original text:"
      ],
      "id": "1ca48061"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "523b9487",
        "outputId": "2026a4c5-4ae8-4c92-f9ad-7e2d81a4d983"
      },
      "source": [
        "test_inv = [2, 3, 4, 5]\n",
        "\n",
        "orig_sentence = \" \".join((inverse_vocabulary.get(i, \"[UNK]\") for i in test_inv))\n",
        "\n",
        "orig_sentence"
      ],
      "id": "523b9487",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is a messed up'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b07d14e"
      },
      "source": [
        "## Putting it all together"
      ],
      "id": "3b07d14e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fa67bb4"
      },
      "source": [
        "# TO DO: complete the make_vocabulary function\n",
        "\n",
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, input_text):\n",
        "        text = input_text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, input_text):\n",
        "        text = self.standardize(input_text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, input_text):\n",
        "        self.vocabulary = {\"[UNK]\": 0}\n",
        "        for text in input_text:\n",
        "            text = self.standardize(input_text)\n",
        "            tokens = self.tokenize(input_text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "\n",
        "    def encode(self, input_text):\n",
        "        text = self.standardize(input_text)\n",
        "        tokens = self.tokenize(input_text)\n",
        "        return [self.vocabulary.get(token, 0) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
      ],
      "id": "8fa67bb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0f66010"
      },
      "source": [
        "Create a sentence and test out all the methods of this vectorizer class."
      ],
      "id": "a0f66010"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63788b5b",
        "outputId": "e6d07c4c-2fdc-418e-8a20-3bb211ca8892"
      },
      "source": [
        "v1 = Vectorizer()\n",
        "\n",
        "input_text = \"THIs iS A meSSeD, uP SeNTenCE!.\"\n",
        "\n",
        "text = v1.standardize(input_text)\n",
        "print(text)\n",
        "\n",
        "tokens = v1.tokenize(input_text)\n",
        "print(tokens)\n",
        "\n",
        "inv_vocab = v1.make_vocabulary(input_text)\n",
        "print(inv_vocab)\n",
        "\n",
        "encode = v1.encode(input_text)\n",
        "print(encode)\n",
        "\n",
        "int_sequence = [2,4,6]\n",
        "\n",
        "decode = v1.decode(int_sequence)\n",
        "print(decode)\n"
      ],
      "id": "63788b5b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a messed up sentence\n",
            "['this', 'is', 'a', 'messed', 'up', 'sentence']\n",
            "None\n",
            "[1, 2, 3, 4, 5, 6]\n",
            "is messed sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33635a6a"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "Now create a function that will one-hot encode each token of a sentence, once a vocabulary has been created."
      ],
      "id": "33635a6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4541bf60",
        "outputId": "5d61a066-87fe-4098-b1d6-32c84b6ba773"
      },
      "source": [
        "input_text = 'The cat is on the car!'\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "\n",
        "vectorizer.make_vocabulary(input_text)\n",
        "\n",
        "\n",
        "vocabulary = vectorizer.vocabulary\n",
        "vocabulary"
      ],
      "id": "4541bf60",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'[UNK]': 0, 'the': 1, 'cat': 2, 'is': 3, 'on': 4, 'car': 5}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ec4308"
      },
      "source": [
        "# TO DO: complete the function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encode_token(token):\n",
        "    vector = np.zeros(len(vocabulary),)\n",
        "    vector[token] = 1\n",
        "    return vector\n",
        ""
      ],
      "id": "07ec4308",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040d3580",
        "outputId": "23a61ed6-ceed-4698-85ac-d66e9ef8d8e4"
      },
      "source": [
        "int_vec = v1.encode('The cat is on the car!')\n",
        "int_vec"
      ],
      "id": "040d3580",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 2, 0, 0, 0]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c01416f2",
        "outputId": "0bc7fa46-9543-4353-d3cb-dac7e3763193"
      },
      "source": [
        "one_hot_encoded = [one_hot_encode_token(token) for token in int_vec]\n",
        "one_hot_encoded"
      ],
      "id": "c01416f2",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([1., 0., 0., 0., 0., 0.]),\n",
              " array([1., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 1., 0., 0., 0.]),\n",
              " array([1., 0., 0., 0., 0., 0.]),\n",
              " array([1., 0., 0., 0., 0., 0.]),\n",
              " array([1., 0., 0., 0., 0., 0.])]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ac667b"
      },
      "source": [
        "## Using Keras\n",
        "\n",
        "Using the [TextVectorization](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/) layer in Keras:"
      ],
      "id": "44ac667b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d787a3d3"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(output_mode=\"int\")"
      ],
      "id": "d787a3d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82c5bdd0"
      },
      "source": [
        "sentence = \"The cat sat on the mat\"\n",
        "\n",
        "text_vectorization.adapt([sentence])"
      ],
      "id": "82c5bdd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcf0f37",
        "outputId": "4ec15464-8e29-4c92-8248-5fe3240411d1"
      },
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "id": "0fcf0f37",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'sat', 'on', 'mat', 'cat']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc1d9b2e",
        "outputId": "194e6a9f-fec7-4d66-aecd-07a8f53fb71c"
      },
      "source": [
        "new_sentence = \"The cat did not sit on the mat\"\n",
        "\n",
        "text_vectorization([new_sentence]).numpy()"
      ],
      "id": "fc1d9b2e",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2, 6, 1, 1, 1, 4, 2, 5]], dtype=int64)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdb34649"
      },
      "source": [
        "### N-grams"
      ],
      "id": "bdb34649"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7215877b"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization_bigram = TextVectorization(ngrams=2, output_mode=\"int\")"
      ],
      "id": "7215877b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aa79fb7"
      },
      "source": [
        "sentence = \"The cat sat on the mat\"\n",
        "\n",
        "text_vectorization_bigram.adapt([sentence])"
      ],
      "id": "1aa79fb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a39ce6b7",
        "outputId": "eeefa441-2c44-4e81-b393-707f5e2ab039"
      },
      "source": [
        "text_vectorization_bigram.get_vocabulary()"
      ],
      "id": "a39ce6b7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'the mat',\n",
              " 'the cat',\n",
              " 'sat on',\n",
              " 'sat',\n",
              " 'on the',\n",
              " 'on',\n",
              " 'mat',\n",
              " 'cat sat',\n",
              " 'cat']"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b16086ce",
        "outputId": "7d1743e9-c947-434b-ebdd-4247a91b00df"
      },
      "source": [
        "new_sentence = \"The cat did not sit on the mat\"\n",
        "\n",
        "text_vectorization_bigram([new_sentence]).numpy()"
      ],
      "id": "b16086ce",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 2, 11,  1,  1,  1,  8,  2,  9,  4,  1,  1,  1,  1,  7,  3]],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ddd25c8"
      },
      "source": [
        "### One-hot encoding"
      ],
      "id": "3ddd25c8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cdaedc7",
        "outputId": "1fb525d9-4f5a-43ed-fe5e-a7d2c4655d41"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vectorized_text = text_vectorization([\"The cat did not sit on the mat\"])\n",
        "vectorized_text\n",
        "\n",
        "one_hot = tf.one_hot(vectorized_text, depth=8)\n",
        "one_hot.numpy()"
      ],
      "id": "7cdaedc7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a1790b"
      },
      "source": [
        "### Learning embeddings"
      ],
      "id": "a5a1790b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d01039be",
        "outputId": "429e0f69-3ada-4cf5-e08d-bb3561eb8694"
      },
      "source": [
        "vectorized_text = text_vectorization([\"The cat did not sit on the mat\"])\n",
        "vectorized_text.numpy()"
      ],
      "id": "d01039be",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2, 6, 1, 1, 1, 4, 2, 5]], dtype=int64)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "378153e7",
        "outputId": "1df6c121-26e0-42c3-e477-9b5756f6831c"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedded = Embedding(input_dim=8, output_dim=4)(vectorized_text)\n",
        "embedded.numpy()"
      ],
      "id": "378153e7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[-0.01100979,  0.02564209,  0.04117367,  0.04404965],\n",
              "        [-0.02440178, -0.03419872, -0.04710355, -0.02102966],\n",
              "        [ 0.04030227,  0.01847314,  0.02640769,  0.01122208],\n",
              "        [ 0.04030227,  0.01847314,  0.02640769,  0.01122208],\n",
              "        [ 0.04030227,  0.01847314,  0.02640769,  0.01122208],\n",
              "        [-0.04717664, -0.01785877, -0.03373032,  0.0059191 ],\n",
              "        [-0.01100979,  0.02564209,  0.04117367,  0.04404965],\n",
              "        [ 0.00040773, -0.04087483, -0.01782596,  0.0468341 ]]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ea7da3f"
      },
      "source": [
        "### Pretrained embeddings"
      ],
      "id": "2ea7da3f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fc85ffc"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "7fc85ffc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe5284f3"
      },
      "source": [
        "glove = []\n",
        "\n",
        "with open(\"glove.6B.50d.txt\") as file:\n",
        "    i = 0\n",
        "    for line in file:\n",
        "        glove.append(line.rstrip())\n",
        "        i += 1\n",
        "        if i > 20: break"
      ],
      "id": "fe5284f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539e2639"
      },
      "source": [
        "glove_dict ={}\n",
        "\n",
        "for word in glove:\n",
        "    vec = word.split()\n",
        "    glove_dict[vec[0]] = vec[1:]"
      ],
      "id": "539e2639",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d85f14ae",
        "outputId": "e4d5527e-47c8-4420-d45c-0520304f3a55"
      },
      "source": [
        "glove_df = pd.DataFrame(data=glove_dict).transpose()\n",
        "glove_df.sort_index(axis=0)[5:]"
      ],
      "id": "d85f14ae",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.21705</td>\n",
              "      <td>0.46515</td>\n",
              "      <td>-0.46757</td>\n",
              "      <td>0.10082</td>\n",
              "      <td>1.0135</td>\n",
              "      <td>0.74845</td>\n",
              "      <td>-0.53104</td>\n",
              "      <td>-0.26256</td>\n",
              "      <td>0.16812</td>\n",
              "      <td>0.13182</td>\n",
              "      <td>...</td>\n",
              "      <td>0.13813</td>\n",
              "      <td>0.36973</td>\n",
              "      <td>-0.64289</td>\n",
              "      <td>0.024142</td>\n",
              "      <td>-0.039315</td>\n",
              "      <td>-0.26037</td>\n",
              "      <td>0.12017</td>\n",
              "      <td>-0.043782</td>\n",
              "      <td>0.41013</td>\n",
              "      <td>0.1796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.26818</td>\n",
              "      <td>0.14346</td>\n",
              "      <td>-0.27877</td>\n",
              "      <td>0.016257</td>\n",
              "      <td>0.11384</td>\n",
              "      <td>0.69923</td>\n",
              "      <td>-0.51332</td>\n",
              "      <td>-0.47368</td>\n",
              "      <td>-0.33075</td>\n",
              "      <td>-0.13834</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069043</td>\n",
              "      <td>0.36885</td>\n",
              "      <td>0.25168</td>\n",
              "      <td>-0.24517</td>\n",
              "      <td>0.25381</td>\n",
              "      <td>0.1367</td>\n",
              "      <td>-0.31178</td>\n",
              "      <td>-0.6321</td>\n",
              "      <td>-0.25028</td>\n",
              "      <td>-0.38097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>as</th>\n",
              "      <td>0.20782</td>\n",
              "      <td>0.12713</td>\n",
              "      <td>-0.30188</td>\n",
              "      <td>-0.23125</td>\n",
              "      <td>0.30175</td>\n",
              "      <td>0.33194</td>\n",
              "      <td>-0.52776</td>\n",
              "      <td>-0.44042</td>\n",
              "      <td>-0.48348</td>\n",
              "      <td>0.03502</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.15768</td>\n",
              "      <td>0.39606</td>\n",
              "      <td>-0.23646</td>\n",
              "      <td>-0.095054</td>\n",
              "      <td>0.07859</td>\n",
              "      <td>-0.012305</td>\n",
              "      <td>-0.49879</td>\n",
              "      <td>-0.35301</td>\n",
              "      <td>0.05058</td>\n",
              "      <td>0.019495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>0.15272</td>\n",
              "      <td>0.36181</td>\n",
              "      <td>-0.22168</td>\n",
              "      <td>0.066051</td>\n",
              "      <td>0.13029</td>\n",
              "      <td>0.37075</td>\n",
              "      <td>-0.75874</td>\n",
              "      <td>-0.44722</td>\n",
              "      <td>0.22563</td>\n",
              "      <td>0.10208</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020339</td>\n",
              "      <td>0.2142</td>\n",
              "      <td>0.044097</td>\n",
              "      <td>0.14003</td>\n",
              "      <td>-0.20079</td>\n",
              "      <td>0.074794</td>\n",
              "      <td>-0.36076</td>\n",
              "      <td>0.43382</td>\n",
              "      <td>-0.084617</td>\n",
              "      <td>0.1214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>he</th>\n",
              "      <td>-0.20092</td>\n",
              "      <td>-0.060271</td>\n",
              "      <td>-0.61766</td>\n",
              "      <td>-0.8444</td>\n",
              "      <td>0.5781</td>\n",
              "      <td>0.14671</td>\n",
              "      <td>-0.86098</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>-0.86556</td>\n",
              "      <td>-0.18234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.16925</td>\n",
              "      <td>0.10228</td>\n",
              "      <td>-0.62143</td>\n",
              "      <td>0.19829</td>\n",
              "      <td>-0.36147</td>\n",
              "      <td>-0.24769</td>\n",
              "      <td>-0.38989</td>\n",
              "      <td>-0.33317</td>\n",
              "      <td>-0.041659</td>\n",
              "      <td>-0.013171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>0.33042</td>\n",
              "      <td>0.24995</td>\n",
              "      <td>-0.60874</td>\n",
              "      <td>0.10923</td>\n",
              "      <td>0.036372</td>\n",
              "      <td>0.151</td>\n",
              "      <td>-0.55083</td>\n",
              "      <td>-0.074239</td>\n",
              "      <td>-0.092307</td>\n",
              "      <td>-0.32821</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.48609</td>\n",
              "      <td>-0.0080272</td>\n",
              "      <td>0.031184</td>\n",
              "      <td>-0.36576</td>\n",
              "      <td>-0.42699</td>\n",
              "      <td>0.42164</td>\n",
              "      <td>-0.11666</td>\n",
              "      <td>-0.50703</td>\n",
              "      <td>-0.027273</td>\n",
              "      <td>-0.53285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>0.6185</td>\n",
              "      <td>0.64254</td>\n",
              "      <td>-0.46552</td>\n",
              "      <td>0.3757</td>\n",
              "      <td>0.74838</td>\n",
              "      <td>0.53739</td>\n",
              "      <td>0.0022239</td>\n",
              "      <td>-0.60577</td>\n",
              "      <td>0.26408</td>\n",
              "      <td>0.11703</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016573</td>\n",
              "      <td>0.312</td>\n",
              "      <td>-0.33189</td>\n",
              "      <td>-0.026001</td>\n",
              "      <td>-0.38203</td>\n",
              "      <td>0.19403</td>\n",
              "      <td>-0.12466</td>\n",
              "      <td>-0.27557</td>\n",
              "      <td>0.30899</td>\n",
              "      <td>0.48497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>it</th>\n",
              "      <td>0.61183</td>\n",
              "      <td>-0.22072</td>\n",
              "      <td>-0.10898</td>\n",
              "      <td>-0.052967</td>\n",
              "      <td>0.50804</td>\n",
              "      <td>0.34684</td>\n",
              "      <td>-0.33558</td>\n",
              "      <td>-0.19152</td>\n",
              "      <td>-0.035865</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050059</td>\n",
              "      <td>-0.10058</td>\n",
              "      <td>-0.017907</td>\n",
              "      <td>0.11142</td>\n",
              "      <td>-0.71798</td>\n",
              "      <td>0.491</td>\n",
              "      <td>-0.099974</td>\n",
              "      <td>-0.043688</td>\n",
              "      <td>-0.097922</td>\n",
              "      <td>0.16806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.70853</td>\n",
              "      <td>0.57088</td>\n",
              "      <td>-0.4716</td>\n",
              "      <td>0.18048</td>\n",
              "      <td>0.54449</td>\n",
              "      <td>0.72603</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.52393</td>\n",
              "      <td>0.10381</td>\n",
              "      <td>-0.17566</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.34727</td>\n",
              "      <td>0.28483</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.38988</td>\n",
              "      <td>0.22902</td>\n",
              "      <td>-0.21617</td>\n",
              "      <td>-0.22562</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.80375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>on</th>\n",
              "      <td>0.30045</td>\n",
              "      <td>0.25006</td>\n",
              "      <td>-0.16692</td>\n",
              "      <td>0.1923</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>-0.079486</td>\n",
              "      <td>-0.91383</td>\n",
              "      <td>-0.1974</td>\n",
              "      <td>-0.053413</td>\n",
              "      <td>-0.40846</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.089032</td>\n",
              "      <td>0.062001</td>\n",
              "      <td>-0.19946</td>\n",
              "      <td>-0.38863</td>\n",
              "      <td>-0.18232</td>\n",
              "      <td>0.060751</td>\n",
              "      <td>0.098603</td>\n",
              "      <td>-0.07131</td>\n",
              "      <td>0.23052</td>\n",
              "      <td>-0.51939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>said</th>\n",
              "      <td>0.38973</td>\n",
              "      <td>-0.2121</td>\n",
              "      <td>0.51837</td>\n",
              "      <td>0.80136</td>\n",
              "      <td>1.0336</td>\n",
              "      <td>-0.27784</td>\n",
              "      <td>-0.84525</td>\n",
              "      <td>-0.25333</td>\n",
              "      <td>0.12586</td>\n",
              "      <td>-0.90342</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.46297</td>\n",
              "      <td>1.3186</td>\n",
              "      <td>0.32705</td>\n",
              "      <td>-0.73446</td>\n",
              "      <td>0.89301</td>\n",
              "      <td>-0.45324</td>\n",
              "      <td>-1.2698</td>\n",
              "      <td>0.86119</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>1.2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>0.88387</td>\n",
              "      <td>-0.14199</td>\n",
              "      <td>0.13566</td>\n",
              "      <td>0.098682</td>\n",
              "      <td>0.51218</td>\n",
              "      <td>0.49138</td>\n",
              "      <td>-0.47155</td>\n",
              "      <td>-0.30742</td>\n",
              "      <td>0.01963</td>\n",
              "      <td>0.12686</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.35283</td>\n",
              "      <td>0.44882</td>\n",
              "      <td>-0.16534</td>\n",
              "      <td>0.31579</td>\n",
              "      <td>0.14963</td>\n",
              "      <td>-0.071277</td>\n",
              "      <td>-0.53506</td>\n",
              "      <td>0.52711</td>\n",
              "      <td>-0.20148</td>\n",
              "      <td>0.0095952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418</td>\n",
              "      <td>0.24968</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>0.34527</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.17862</td>\n",
              "      <td>-0.00066023</td>\n",
              "      <td>-0.6566</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.29871</td>\n",
              "      <td>-0.15749</td>\n",
              "      <td>-0.34758</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.44251</td>\n",
              "      <td>0.18785</td>\n",
              "      <td>0.0027849</td>\n",
              "      <td>-0.18411</td>\n",
              "      <td>-0.11514</td>\n",
              "      <td>-0.78581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.68047</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.17792</td>\n",
              "      <td>0.42962</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.13228</td>\n",
              "      <td>-0.29847</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.21048</td>\n",
              "      <td>-0.03088</td>\n",
              "      <td>-0.19722</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.09434</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.26044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>was</th>\n",
              "      <td>0.086888</td>\n",
              "      <td>-0.19416</td>\n",
              "      <td>-0.24267</td>\n",
              "      <td>-0.33391</td>\n",
              "      <td>0.56731</td>\n",
              "      <td>0.39783</td>\n",
              "      <td>-0.97809</td>\n",
              "      <td>0.03159</td>\n",
              "      <td>-0.61469</td>\n",
              "      <td>-0.31406</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1201</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>-0.43998</td>\n",
              "      <td>-0.48531</td>\n",
              "      <td>-0.5188</td>\n",
              "      <td>-0.3077</td>\n",
              "      <td>-0.75028</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>0.3945</td>\n",
              "      <td>-0.16937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>with</th>\n",
              "      <td>0.25616</td>\n",
              "      <td>0.43694</td>\n",
              "      <td>-0.11889</td>\n",
              "      <td>0.20345</td>\n",
              "      <td>0.41959</td>\n",
              "      <td>0.85863</td>\n",
              "      <td>-0.60344</td>\n",
              "      <td>-0.31835</td>\n",
              "      <td>-0.6718</td>\n",
              "      <td>0.003984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.13691</td>\n",
              "      <td>0.37762</td>\n",
              "      <td>-0.12159</td>\n",
              "      <td>-0.13808</td>\n",
              "      <td>0.19505</td>\n",
              "      <td>0.22793</td>\n",
              "      <td>-0.17304</td>\n",
              "      <td>-0.07573</td>\n",
              "      <td>-0.25868</td>\n",
              "      <td>-0.39339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0          1         2          3         4          5   \\\n",
              "a      0.21705    0.46515  -0.46757    0.10082    1.0135    0.74845   \n",
              "and    0.26818    0.14346  -0.27877   0.016257   0.11384    0.69923   \n",
              "as     0.20782    0.12713  -0.30188   -0.23125   0.30175    0.33194   \n",
              "for    0.15272    0.36181  -0.22168   0.066051   0.13029    0.37075   \n",
              "he    -0.20092  -0.060271  -0.61766    -0.8444    0.5781    0.14671   \n",
              "in     0.33042    0.24995  -0.60874    0.10923  0.036372      0.151   \n",
              "is      0.6185    0.64254  -0.46552     0.3757   0.74838    0.53739   \n",
              "it     0.61183   -0.22072  -0.10898  -0.052967   0.50804    0.34684   \n",
              "of     0.70853    0.57088   -0.4716    0.18048   0.54449    0.72603   \n",
              "on     0.30045    0.25006  -0.16692     0.1923  0.026921  -0.079486   \n",
              "said   0.38973    -0.2121   0.51837    0.80136    1.0336   -0.27784   \n",
              "that   0.88387   -0.14199   0.13566   0.098682   0.51218    0.49138   \n",
              "the      0.418    0.24968  -0.41242     0.1217   0.34527  -0.044457   \n",
              "to     0.68047  -0.039263   0.30186   -0.17792   0.42962   0.032246   \n",
              "was   0.086888   -0.19416  -0.24267   -0.33391   0.56731    0.39783   \n",
              "with   0.25616    0.43694  -0.11889    0.20345   0.41959    0.85863   \n",
              "\n",
              "             6          7            8          9   ...         40  \\\n",
              "a      -0.53104   -0.26256      0.16812    0.13182  ...    0.13813   \n",
              "and    -0.51332   -0.47368     -0.33075   -0.13834  ...  -0.069043   \n",
              "as     -0.52776   -0.44042     -0.48348    0.03502  ...   -0.15768   \n",
              "for    -0.75874   -0.44722      0.22563    0.10208  ...   0.020339   \n",
              "he     -0.86098     0.6705     -0.86556   -0.18234  ...   -0.16925   \n",
              "in     -0.55083  -0.074239    -0.092307   -0.32821  ...   -0.48609   \n",
              "is    0.0022239   -0.60577      0.26408    0.11703  ...  -0.016573   \n",
              "it     -0.33558   -0.19152    -0.035865     0.1051  ...   0.050059   \n",
              "of      0.18157   -0.52393      0.10381   -0.17566  ...   -0.34727   \n",
              "on     -0.91383    -0.1974    -0.053413   -0.40846  ...  -0.089032   \n",
              "said   -0.84525   -0.25333      0.12586   -0.90342  ...   -0.46297   \n",
              "that   -0.47155   -0.30742      0.01963    0.12686  ...   -0.35283   \n",
              "the    -0.49688   -0.17862  -0.00066023    -0.6566  ...   -0.29871   \n",
              "to     -0.41376    0.13228     -0.29847  -0.085253  ...  -0.094375   \n",
              "was    -0.97809    0.03159     -0.61469   -0.31406  ...    -0.1201   \n",
              "with   -0.60344   -0.31835      -0.6718   0.003984  ...    0.13691   \n",
              "\n",
              "              41         42         43         44         45         46  \\\n",
              "a        0.36973   -0.64289   0.024142  -0.039315   -0.26037    0.12017   \n",
              "and      0.36885    0.25168   -0.24517    0.25381     0.1367   -0.31178   \n",
              "as       0.39606   -0.23646  -0.095054    0.07859  -0.012305   -0.49879   \n",
              "for       0.2142   0.044097    0.14003   -0.20079   0.074794   -0.36076   \n",
              "he       0.10228   -0.62143    0.19829   -0.36147   -0.24769   -0.38989   \n",
              "in    -0.0080272   0.031184   -0.36576   -0.42699    0.42164   -0.11666   \n",
              "is         0.312   -0.33189  -0.026001   -0.38203    0.19403   -0.12466   \n",
              "it      -0.10058  -0.017907    0.11142   -0.71798      0.491  -0.099974   \n",
              "of       0.28483   0.075693  -0.062178   -0.38988    0.22902   -0.21617   \n",
              "on      0.062001   -0.19946   -0.38863   -0.18232   0.060751   0.098603   \n",
              "said      1.3186    0.32705   -0.73446    0.89301   -0.45324    -1.2698   \n",
              "that     0.44882   -0.16534    0.31579    0.14963  -0.071277   -0.53506   \n",
              "the     -0.15749   -0.34758  -0.045637   -0.44251    0.18785  0.0027849   \n",
              "to      0.018324    0.21048   -0.03088   -0.19722   0.082279   -0.09434   \n",
              "was     0.096132   -0.43998   -0.48531    -0.5188    -0.3077   -0.75028   \n",
              "with     0.37762   -0.12159   -0.13808    0.19505    0.22793   -0.17304   \n",
              "\n",
              "             47         48         49  \n",
              "a     -0.043782    0.41013     0.1796  \n",
              "and     -0.6321   -0.25028   -0.38097  \n",
              "as     -0.35301    0.05058   0.019495  \n",
              "for     0.43382  -0.084617     0.1214  \n",
              "he     -0.33317  -0.041659  -0.013171  \n",
              "in     -0.50703  -0.027273   -0.53285  \n",
              "is     -0.27557    0.30899    0.48497  \n",
              "it    -0.043688  -0.097922    0.16806  \n",
              "of     -0.22562  -0.093918   -0.80375  \n",
              "on     -0.07131    0.23052   -0.51939  \n",
              "said    0.86119     0.1415     1.2018  \n",
              "that    0.52711   -0.20148  0.0095952  \n",
              "the    -0.18411   -0.11514   -0.78581  \n",
              "to    -0.073297  -0.064699   -0.26044  \n",
              "was       -0.77     0.3945   -0.16937  \n",
              "with   -0.07573   -0.25868   -0.39339  \n",
              "\n",
              "[16 rows x 50 columns]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fddc7086"
      },
      "source": [],
      "id": "fddc7086",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK_5ug_b6TgG"
      },
      "source": [
        " class Vectorizer:\n",
        "    def standardize(self, input_text):\n",
        "        text = input_text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, input_text):\n",
        "        text = self.standardize(input_text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, input_text):\n",
        "        self.vocabulary = {\"[UNK]\": 0}\n",
        "        for text in input_text:\n",
        "            text = self.standardize(input_text)\n",
        "            tokens = self.tokenize(input_text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "\n",
        "    def encode(self, input_text):\n",
        "        text = self.standardize(input_text)\n",
        "        tokens = self.tokenize(input_text)\n",
        "        return [self.vocabulary.get(token, 0) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "v1 = Vectorizer()\n",
        "x = x.apply(lambda x : x.to_string())\n",
        "vocabulary = x.apply(lambda x : v1.make_vocabulary(x))\n",
        "encode = x.apply(lambda x : v1.encode(x))"
      ],
      "id": "CK_5ug_b6TgG",
      "execution_count": null,
      "outputs": []
    }
  ]
}